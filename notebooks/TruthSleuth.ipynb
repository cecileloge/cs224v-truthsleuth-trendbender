{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cecileloge/cs224v-truthsleuth-trendbender/blob/main/notebooks/TruthSleuth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Truth Sleuth AI: Fact-Checker Agent for YouTube Videos**\n",
        "**Cecile Loge ep. Baccari** | ceciloge@stanford.edu | cecileloge@google.com \\\\\n",
        "**Mohammad Rehan Ghori** | rghori@stanford.edu | rehang@google.com \\\\\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Motivation:** Misinformation is one of the most pressing threats of our time, and YouTube videos serve as a major platform through which it can spread [1]. Providing fact-checked information to address misleading content has been shown to be more effective than simply removing it [2].\n",
        "\n",
        "**Project:** Can we build an application that takes a YouTube video as input and not only generates a list of the main claims made in the video but also fact-checks them?\n",
        "\n",
        "---\n",
        "\n",
        "* [1] An open letter to YouTube‚Äôs CEO from the world‚Äôs fact-checkers (on poynter.org), 2022. \\\\\n",
        "* [2] Ecker, Ullrich KH, et al. \"The effectiveness of short‚Äêformat refutational fact‚Äêchecks.\" British journal of psychology 111.1 (2020): 36-54."
      ],
      "metadata": {
        "id": "W_jcGO58h939"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Setting Up Everything**\n",
        "Choosing the YouTube video url, and installing/importing libraries.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "iOdGDcTj2dg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide the video url\n",
        "VIDEO_URL = \"https://www.youtube.com/watch?v=ssqucRUoRjs\""
      ],
      "metadata": {
        "id": "o0pvBC3Ng5zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPwx_gkvhCLJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Install & Import Libraries\n",
        "\n",
        "# Youtube Extractors\n",
        "!pip install youtube-transcript-api\n",
        "!pip install pytube\n",
        "!pip install -U yt-dlp\n",
        "!apt install ffmpeg\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from pytube import extract\n",
        "\n",
        "# Assembly AI\n",
        "!pip install assemblyai\n",
        "import assemblyai as aai\n",
        "\n",
        "# Data & Tools\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "current_dir = os.getcwd()\n",
        "from datetime import datetime, date\n",
        "import time\n",
        "\n",
        "# Google API\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "# Markdown for the final output\n",
        "from IPython.display import display, Markdown, Latex\n",
        "import textwrap\n",
        "\n",
        "# Gemini API\n",
        "!pip install -q -U google-generativeai\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# LangChain Prompting\n",
        "!pip install langchain\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# For Web Scraping\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install wikipedia\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import wikipedia\n",
        "import googlesearch as g"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YouTube API Key (Google for Developers Platform)\n",
        "DEVELOPER_KEY=userdata.get('DEVELOPER_KEY')\n",
        "\n",
        "# Google Developer API Key for GenAI\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
        "GEM_SAFETY_SETTINGS = [\n",
        "    {\n",
        "      \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "      \"threshold\": \"BLOCK_NONE\"\n",
        "    },\n",
        "    {\n",
        "      \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "      \"threshold\": \"BLOCK_NONE\"\n",
        "    },\n",
        "    {\n",
        "      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "      \"threshold\": \"BLOCK_NONE\"\n",
        "    },\n",
        "    {\n",
        "      \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "      \"threshold\": \"BLOCK_NONE\"\n",
        "    }\n",
        "    ]\n",
        "\n",
        "# Assembly AI API Key\n",
        "AAI_API_KEY = userdata.get('AAI_API_KEY')\n",
        "aai.settings.api_key = AAI_API_KEY"
      ],
      "metadata": {
        "id": "-YQmckZyELCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt Templates\n",
        "!mkdir prompts\n",
        "!curl -L -o prompts/reformat.prompt \"https://drive.google.com/uc?export=download&id=1aykUMXxUR1gWOil5X83aF7aem1s5rjIT\"\n",
        "!curl -L -o prompts/claims.prompt \"https://drive.google.com/uc?export=download&id=1AlKLI-IP05jMps4Ol7BN2RPcmAMH2H-J\"\n",
        "!curl -L -o prompts/factcheck.prompt \"https://drive.google.com/uc?export=download&id=1cevbk44t7ZWJr3rwpu-b1ncqHYdq-p8x\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mvGIxf1Jt6S",
        "outputId": "49f01178-25e4-4979-dd51-e6ab45e74a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‚Äòprompts‚Äô: File exists\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100   224  100   224    0     0     81      0  0:00:02  0:00:02 --:--:--    96\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  8614  100  8614    0     0   3436      0  0:00:02  0:00:02 --:--:--  8562\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  4630  100  4630    0     0   1590      0  0:00:02  0:00:02 --:--:--  3680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **STEP 1 | Extracting info & audio from Video URL**\n",
        "\n",
        "Functions to process a video from a provided YouTube link. Should output a text transcript from the audio - along with descriptions of the video (title, author, tags) and a summary of the comments.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FFigO_1tsQld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_comments(video_id):\n",
        "  \"\"\"\n",
        "  Function to get top 100 comments from a YouTube video.\n",
        "  Saves them into comments.csv. Returns a panda dataframe.\n",
        "  \"\"\"\n",
        "  youtube = build(\"youtube\", \"v3\", developerKey=DEVELOPER_KEY)\n",
        "\n",
        "  try:\n",
        "    # Retrieve comment thread using the youtube.commentThreads().list() method\n",
        "    response = youtube.commentThreads().list(\n",
        "        part=\"snippet\",\n",
        "        videoId=video_id,\n",
        "        maxResults=100,\n",
        "        order=\"relevance\"\n",
        "    ).execute()\n",
        "\n",
        "    comments = []\n",
        "    for item in response[\"items\"]:\n",
        "      comment_text = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "      likes = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n",
        "      comments.append({\"comment\": comment_text, \"num_of_likes\": likes})\n",
        "\n",
        "      #if 'nextPageToken' in response:\n",
        "      #  response = youtube.commentThreads().list(\n",
        "      #      part=\"snippet\",\n",
        "      #      videoId=video_id,\n",
        "      #      maxResults=100,\n",
        "      #      order=\"relevance\"\n",
        "      #      pageToken = response['nextPageToken']\n",
        "      #  ).execute()\n",
        "      #else:\n",
        "      #  break\n",
        "      comments_df = pd.DataFrame(comments).sort_values(by=['num_of_likes'], ascending=False)\n",
        "      comments_df.to_csv(\"comments.csv\", index=False)\n",
        "    return comments_df\n",
        "  except HttpError as error:\n",
        "    print(f\"An HTTP error {error.http_status} occurred:\\n {error.content}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "FM03mM0F3coO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_details(video_id):\n",
        "  \"\"\"\n",
        "  Function to get details from a YouTube video.\n",
        "  Returns a tuple for title, channel, tags, views, likes.\n",
        "  \"\"\"\n",
        "  youtube = build('youtube', 'v3', developerKey=DEVELOPER_KEY)\n",
        "  request = youtube.videos().list(part='snippet,statistics', id=video_id)\n",
        "  details = request.execute()\n",
        "  thumbnail_url = details['items'][0]['snippet']['thumbnails']['high']['url']\n",
        "  channel = details['items'][0]['snippet']['channelTitle']\n",
        "  title = details['items'][0]['snippet']['title']\n",
        "  tags = details['items'][0]['snippet'].get('tags')\n",
        "  likes = int(details['items'][0]['statistics']['likeCount'])\n",
        "  views = int(details['items'][0]['statistics']['viewCount'])\n",
        "  videodate = details['items'][0]['snippet']['publishedAt']\n",
        "  return channel, title, tags, likes, views, thumbnail_url, videodate\n"
      ],
      "metadata": {
        "id": "V52WW_sXqlM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_captions(video_url, video_id):\n",
        "  \"\"\"\n",
        "  Function to get audio captions in 'en' (English) from a YouTube video.\n",
        "  Either from the YouTube subtitles if they exist, or from Assembly AI.\n",
        "  Uses Gemini to format the raw audio captions, and returns a string.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    yt = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
        "    captions = ''\n",
        "    for i in yt:\n",
        "      captions += i['text']+\" \"\n",
        "  except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(f\"Using Assembly AI instead...\")\n",
        "    !yt-dlp --get-url -f bestaudio $VIDEO_URL > audio.txt\n",
        "    with open('audio.txt', 'r') as file:\n",
        "      AUDIO_URL = file.read()\n",
        "    config = aai.TranscriptionConfig(auto_highlights=True)\n",
        "    transcriber = aai.Transcriber()\n",
        "    transcript = transcriber.transcribe(AUDIO_URL, config)\n",
        "    captions = transcript.text\n",
        "\n",
        "  # Using Gemini to format the raw audio captions\n",
        "  with open(\"prompts/reformat.prompt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "  prompt_template = PromptTemplate.from_template(template=text, template_format=\"jinja2\")\n",
        "  prompt: str = prompt_template.format(captions=captions)\n",
        "  response = model.generate_content(prompt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "  audio_captions_formatted = response.text\n",
        "\n",
        "  return audio_captions_formatted"
      ],
      "metadata": {
        "id": "bxpHDLCunUQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **STEP 2 | Extracting the claims to fact-check from Video audio**\n",
        "\n",
        "Functions to extract the top claims made in the video. We will be using Google's Gemini with robust prompt engineering - leveraging the LangChain library.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JLJ29nrg5UHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_claims(video_url):\n",
        "  \"\"\"\n",
        "  Function to extract the top claims that should be fact-checked.\n",
        "  Uses Gemini with the claims.prompt prompt.\n",
        "  Returns a tuple with: title, channel, thumbnail_url,\n",
        "      and a json object with fields 'claim', 'questions', 'passage', 'relevance'\n",
        "  \"\"\"\n",
        "  video_id = extract.video_id(video_url)\n",
        "  channel, title, _, _, _, thumbnail_url, videodate = get_video_details(video_id)\n",
        "  videodate = datetime.strptime(videodate[:10], '%Y-%m-%d').strftime(\"%Y-%m-%d\")\n",
        "  audio_captions_formatted = get_captions(video_url, video_id)\n",
        "\n",
        "  # Using the prompt template and calling Gemini\n",
        "  with open(\"prompts/claims.prompt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(template=text, template_format=\"jinja2\")\n",
        "  claims_prompt: str = prompt_template.format(\n",
        "      todaydate=date.today().strftime(\"%Y-%m-%d\"),\n",
        "      videodate=videodate,\n",
        "      channel=channel,\n",
        "      title=title,\n",
        "      captions=audio_captions_formatted,\n",
        "      )\n",
        "  response = model.generate_content(claims_prompt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "\n",
        "  json_promt = \"Make sure the following text can be read directly by json.loads(): <<< \" + response.text + \" >>>. '\\\n",
        "  Don't output anything else than your version of the text.\"\n",
        "  response = model.generate_content(json_promt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "  claims = json.loads(response.text[8:-4])\n",
        "\n",
        "  return title, channel, thumbnail_url, claims"
      ],
      "metadata": {
        "id": "M4xkC85HC0h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **STEP 3 | Fact-checking the claims by cross-checking reliable sources**\n",
        "\n",
        "Cross-reference claims with reliable sources, and classify claims into true, unsure and false (ideally with links / sources).\n",
        "\n",
        "\n",
        "This step leverages the Google FactCheck Claim Search API, the Wikipedia API and the Google Search API.\n",
        "\n",
        "The LLM is called several times throughout this step via robust prompt engineering to interpret, cross-reference, and ultimately classify the claims.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "D_LDf7hiDUAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our reliable sources for Fact-Checking\n",
        "# 1 - Google Fact-Check API\n",
        "GOOGLE_FACT_CHECK_API_KEY = userdata.get('GFC_API_KEY')\n",
        "GOOGLE_FACT_CHECK_URL = 'https://factchecktools.googleapis.com/v1alpha1/claims:search'\n",
        "\n",
        "# 2 - Wikipedia\n",
        "wikipedia.set_lang('en')\n",
        "\n",
        "# 3 - Google Search\n",
        "GOOGLE_USER_AGENT = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36\""
      ],
      "metadata": {
        "id": "gufqI6AqsYK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_googlefacts(claim):\n",
        "  \"\"\"\n",
        "  Function to call the Google Fact Check API\n",
        "  Returns the claim reviews in the Claim Review structured format\n",
        "  https://developers.google.com/search/docs/appearance/structured-data/factcheck\n",
        "  \"\"\"\n",
        "\n",
        "  params = {\n",
        "      'query': claim,\n",
        "      'key': GOOGLE_FACT_CHECK_API_KEY\n",
        "  }\n",
        "  response = requests.get(GOOGLE_FACT_CHECK_URL, params=params)\n",
        "  if response.status_code == 200:\n",
        "      data = response.json()\n",
        "      return data.get('claims', [])\n",
        "  else:\n",
        "      return None\n",
        "\n",
        "def check_googlefacts(claim):\n",
        "  \"\"\"\n",
        "  Function to process the Google Fact Check claim reviews\n",
        "  Returns a string = concatenation of relevant results\n",
        "  \"\"\"\n",
        "  results = call_googlefacts(claim)\n",
        "  summary = ''\n",
        "  if results:\n",
        "      for i, r in enumerate(results):\n",
        "          source = r.get('claimReview', [{}])[0].get('publisher').get('name')\n",
        "          url = r.get('claimReview', [{}])[0].get('url')\n",
        "          claimant = r.get('claimant')\n",
        "          date = r.get('claimDate')\n",
        "          text = r.get('text')\n",
        "          truthfulness = r.get('claimReview', [{}])[0].get('textualRating')\n",
        "          summary += f\"Source #{i+1}: {source} at {url}.\\n     Claimant: {claimant}\\n     Description: {text}\\n     Truthfulness: {truthfulness}\\n\\n\"\n",
        "\n",
        "  return summary\n",
        "\n",
        "def check_googlesearch(claim, limit=2):\n",
        "  \"\"\"\n",
        "  Function to call the Google Search API\n",
        "  Calls Gemini to process the results and generate a summary\n",
        "  Returns a string = concatenation of relevant results\n",
        "  \"\"\"\n",
        "  urls = list(g.search(claim, stop=limit, lang='en'))\n",
        "  headers = {\"user-agent\": GOOGLE_USER_AGENT}\n",
        "  summary = ''\n",
        "  for url in urls:\n",
        "    session = requests.Session()\n",
        "    website = session.get(url, headers=headers)\n",
        "    web_soup = BeautifulSoup(website.text, 'html.parser')\n",
        "    summary_promt = \"I need you to give me the key information contained in a specific webpage article. I will provide you with html code. '\\\n",
        "    Do not describe the webpage or the article. Focus on extracting the key claims, and summarizing the information the page is giving in a precise, comprehensive yet concise way. '\\\n",
        "    Your answer should ideally help answer the question: \" + claim + \".\\n'\\\n",
        "    HTLM Code: <<< \" + str(web_soup) + \" >>>\"\n",
        "    retry_count = 0\n",
        "    while retry_count < 3:\n",
        "      try:\n",
        "         response = model.generate_content(summary_promt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "         summary += f\"Source: {url}.\\nDescription: {response.text}\\n\"\n",
        "         retry_count = 3\n",
        "      except Exception as e:\n",
        "         time.sleep(2)\n",
        "      retry_count += 1\n",
        "    session.close()\n",
        "  return summary\n",
        "\n",
        "def check_wikipedia(claim):\n",
        "  \"\"\"\n",
        "  Function to call the Wikipedia API and process the results\n",
        "  Returns a string = concatenation/summary of relevant Wikipedia articles\n",
        "  \"\"\"\n",
        "  summary = ''\n",
        "  search_results = wikipedia.search(claim)\n",
        "  for r in search_results[:2]:\n",
        "    try:\n",
        "      call = wikipedia.page(r)\n",
        "      summary += f\"From the \\\"{r}\\\" Wikipedia page ({call.url}): \"+ call.content +\"\\n\\n\"\n",
        "    except wikipedia.exceptions.DisambiguationError:\n",
        "      summary += ''\n",
        "    except wikipedia.exceptions.PageError:\n",
        "      summary += ''\n",
        "    except wikipedia.exceptions.WikipediaException:\n",
        "      summary += ''\n",
        "    except Exception:\n",
        "      summary += ''\n",
        "  return summary"
      ],
      "metadata": {
        "id": "sANlwtp9XIT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_claim_summary(claim, questions):\n",
        "  \"\"\"\n",
        "  Function to get the final say on a specific claim.\n",
        "  Calls Gemini with the factcheck.prompt prompt.\n",
        "  Returns json object with fields: `claim`, `verdict`, `reason`, `sources`.\n",
        "  \"\"\"\n",
        "  summary_gfc = ''\n",
        "  summary_search = ''\n",
        "  for q in questions:\n",
        "    gfc = check_googlefacts(q)\n",
        "    if gfc:\n",
        "      summary_gfc += gfc + \"\\n\"\n",
        "  for q in questions:\n",
        "    search = check_googlesearch(q)\n",
        "    summary_search += search + \"\\n\"\n",
        "  summary_wiki = check_wikipedia(claim)\n",
        "\n",
        "  with open(\"prompts/factcheck.prompt\", \"r\") as f:\n",
        "        text = f.read()\n",
        "  prompt_template = PromptTemplate.from_template(template=text, template_format=\"jinja2\")\n",
        "  fact_check_prompt: str = prompt_template.format(\n",
        "      claim=claim,\n",
        "      report_GFC=summary_gfc + \"\\n\" + summary_search,\n",
        "      report_wiki=summary_wiki,\n",
        "      )\n",
        "\n",
        "  retry_count = 0\n",
        "  while retry_count < 3:\n",
        "      try:\n",
        "         response = model.generate_content(fact_check_prompt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "         return response.text\n",
        "      except Exception as e:\n",
        "         #print(f\"Error: {e}\")\n",
        "         time.sleep(2)\n",
        "      retry_count += 1\n",
        "  return None"
      ],
      "metadata": {
        "id": "BeYIhFT0hW3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **TRUTH SLEUTH AGENT | Putting it all together & Generating the Fact-Check Report**\n",
        "\n",
        "Generating the final report, leveraging Markdown for formatting. Option to skip printing the \"Unsure\" claims.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "9m5eeMQpFShD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_report(video_url, skip_unsure = True):\n",
        "\n",
        "  # Extracting the claims\n",
        "  title, channel, thumbnail_url, claims = extract_claims(video_url)\n",
        "\n",
        "  # Formatting\n",
        "  color = {\"question\": \"black\", \"true\": \"MediumSpringGreen\", \"partly true\": \"LightGreen\", \"partly false\": \"lightcoral\", \"false\": \"red\", \"unsure\": \"grey\"}\n",
        "  capped = {\"true\": \"TRUE\", \"partly true\": \"PARTLY TRUE\", \"partly false\": \"PARTLY FALSE\", \"false\": \"FALSE\", \"unsure\": \"UNSURE\"}\n",
        "  delimiter = \"\\n\"+\"_\"*100+\"\\n\"\n",
        "\n",
        "  # Generating Report Header\n",
        "  thumb = Image.open(requests.get(thumbnail_url, stream=True).raw)\n",
        "  formatted_title1 = f\"<font size='+2' color='Bisque'><blockquote>üììüìì üîç **TRUTH SLEUTH FACT-CHECK REPORT** üîç üììüìì</blockquote></font>\"\n",
        "  formatted_title2 = f\"<font size='+2' color='white'><blockquote>**{title}** by {channel}</blockquote></font>\"\n",
        "  display(Markdown(formatted_title1 + formatted_title2))\n",
        "  display(thumb)\n",
        "\n",
        "  # Getting each claim's verdict + Formatting again\n",
        "  for i, c in enumerate(claims[\"claims\"]):\n",
        "    text = get_claim_summary(c['claim'], c['questions'])\n",
        "    if text == None:\n",
        "      continue\n",
        "    text = text.replace(\"$\", \"USD \")\n",
        "\n",
        "    try:\n",
        "      verdict = json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "      verdict = json.loads(text[8:-4])\n",
        "    if (verdict.get('verdict') == \"unsure\" and skip_unsure):\n",
        "      continue\n",
        "\n",
        "    formatted_claim = f\"<font size='+1' color='white'><blockquote>**‚Ä¢ {c['claim']}**</blockquote></font>\"\n",
        "    formatted_verdict = f\"<font size='+1' color='{color[verdict.get('verdict')]}'><blockquote>**{capped[verdict.get('verdict')]}**\\n\\n</blockquote></font>\"\n",
        "    links = \"\"\n",
        "    for l in verdict.get('sources'):\n",
        "      links += \"\\n\\n‚Ä¢ \" + l\n",
        "    display(Markdown(delimiter + formatted_claim+formatted_verdict+verdict.get('reason')+links))\n",
        "  display(Markdown(delimiter))\n",
        "\n",
        "  return None"
      ],
      "metadata": {
        "id": "2gk083chvxKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_report(VIDEO_URL, True)"
      ],
      "metadata": {
        "id": "1qAWy_eBMCHx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}