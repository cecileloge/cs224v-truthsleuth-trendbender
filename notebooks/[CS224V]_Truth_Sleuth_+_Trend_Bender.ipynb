{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cecileloge/cs224v-truthsleuth-trendbender/blob/main/notebooks/%5BCS224V%5D_Truth_Sleuth_%2B_Trend_Bender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Truth Sleuth & Trend Bender AI: Multimodal Agent for YouTube Videos**\n",
        "**Mohammad Rehan Ghori** | rghori@stanford.edu | rehang@google.com \\\\\n",
        "**Cecile Loge ep. Baccari** | ceciloge@stanford.edu | cecileloge@google.com\n",
        "\n",
        "---\n",
        "\n",
        "**Motivation:** Misinformation is one of the most pressing threats of our time, and YouTube videos serve as a major platform through which it can spread [1]. Providing fact-checked information to address misleading content has been shown to be more effective than simply removing it [2].\n",
        "\n",
        "\n",
        "\n",
        "**Project:** Can we build an application that takes a YouTube video as input and not only generates a list of the main claims made in the video but also fact-checks them? Ultimately, could that even change people's minds?\n",
        "\n",
        "In particular, could our conversational agent influence the comment section of a video, keep the discussion away from hate/conspiracy and protect people from scams?\n",
        "\n",
        "**Theme 1**: Dangerous Diet Recommendations & Health Claims.\n",
        "**Theme 2**: Manosphere & Misogynistic videos.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "* [1] An open letter to YouTube’s CEO from the world’s fact-checkers (on poynter.org), 2022. \\\\\n",
        "* [2] Ecker, Ullrich KH, et al. \"The effectiveness of short‐format refutational fact‐checks.\" British journal of psychology 111.1 (2020): 36-54."
      ],
      "metadata": {
        "id": "W_jcGO58h939"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Setting Up Everything**\n",
        "Choosing the YouTube video url, and installing/importing libraries.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "iOdGDcTj2dg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide the video url\n",
        "VIDEO_URL = \"https://www.youtube.com/watch?v=8Jl4zm5ftCM\""
      ],
      "metadata": {
        "id": "o0pvBC3Ng5zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPwx_gkvhCLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea860e29-c73e-4232-938a-9a6a8a2a2e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries have been imported / installed! \n",
            ">> You can proceed! :)\n"
          ]
        }
      ],
      "source": [
        "# Install & Import Libraries\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Youtube Extractors\n",
        "!pip install youtube-transcript-api\n",
        "!pip install pytube\n",
        "!pip install -U yt-dlp\n",
        "!apt install ffmpeg\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from pytube import extract\n",
        "\n",
        "# Assembly AI\n",
        "!pip install assemblyai\n",
        "import assemblyai as aai\n",
        "\n",
        "# Data & Tools\n",
        "import pandas as pd\n",
        "import csv\n",
        "from google.colab import userdata\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "current_dir = os.getcwd()\n",
        "from datetime import datetime, date\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from html import unescape\n",
        "\n",
        "# Google API\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "# Markdown for the final output\n",
        "from IPython.display import display, Markdown, Latex\n",
        "import textwrap\n",
        "\n",
        "# Gemini API\n",
        "!pip install -q -U google-generativeai\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# LangChain Prompting\n",
        "!pip install langchain\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# For Web Scraping\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install wikipedia\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import wikipedia\n",
        "import googlesearch as g\n",
        "\n",
        "clear_output()\n",
        "print(\"Libraries have been imported / installed! \\n>> You can proceed! :)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YouTube API Key (Google for Developers Platform)\n",
        "DEVELOPER_KEY=userdata.get('DEVELOPER_KEY')\n",
        "\n",
        "# Google Developer API Key for GenAI\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY2')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\") # \"gemini-1.5-pro\" \"gemini-1.5-flash\"\n",
        "GEM_SAFETY_SETTINGS = [\n",
        "    {\n",
        "      \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "      \"threshold\": \"BLOCK_NONE\"\n",
        "    },\n",
        "    {\n",
        "      \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "      \"threshold\": \"BLOCK_NONE\"\n",
        "    },\n",
        "    {\n",
        "      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "      \"threshold\": \"BLOCK_NONE\"\n",
        "    },\n",
        "    {\n",
        "      \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "      \"threshold\": \"BLOCK_NONE\"\n",
        "    }\n",
        "    ]\n",
        "# Assembly AI API Key\n",
        "AAI_API_KEY = userdata.get('AAI_API_KEY')\n",
        "aai.settings.api_key = AAI_API_KEY\n",
        "\n",
        "print(\"API keys are now set!\")"
      ],
      "metadata": {
        "id": "-YQmckZyELCG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a5d680-e190-44b0-d20f-be5ecdc44393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API keys are now set!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fact-Check Prompt Templates\n",
        "!mkdir prompts\n",
        "!curl -L -o prompts/reformat.prompt \"https://drive.google.com/uc?export=download&id=1aykUMXxUR1gWOil5X83aF7aem1s5rjIT\"\n",
        "!curl -L -o prompts/claims.prompt \"https://drive.google.com/uc?export=download&id=1AlKLI-IP05jMps4Ol7BN2RPcmAMH2H-J\"\n",
        "!curl -L -o prompts/factcheck.prompt \"https://drive.google.com/uc?export=download&id=1cevbk44t7ZWJr3rwpu-b1ncqHYdq-p8x\"\n",
        "!curl -L -o prompts/trend.prompt \"https://drive.google.com/uc?export=download&id=1GylnTltwIXGoVZ-s11EIsMdSuxKIloo6\"\n",
        "clear_output()\n",
        "print(\"Fact-check prompts have been uploaded in the 'prompt' folder and are ready to be used!\")"
      ],
      "metadata": {
        "id": "4mvGIxf1Jt6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596a08c3-ce24-45df-cbb9-87895cace39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fact-check prompts have been uploaded in the 'prompt' folder and are ready to be used!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **STEP 1 | Extracting info & audio from Video URL**\n",
        "\n",
        "Functions to process a video from a provided YouTube link. Should output a text transcript from the audio - along with descriptions of the video (title, author, tags) and a summary of the comments.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FFigO_1tsQld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_comments(video_url):\n",
        "  \"\"\"\n",
        "  Function to get top 100 comments from a YouTube video.\n",
        "  Saves them into comments.csv. Returns a panda dataframe.\n",
        "  \"\"\"\n",
        "  youtube = build(\"youtube\", \"v3\", developerKey=DEVELOPER_KEY)\n",
        "  video_id = extract.video_id(video_url)\n",
        "\n",
        "  try:\n",
        "    # Retrieve comment thread using the youtube.commentThreads().list() method\n",
        "    response = youtube.commentThreads().list(\n",
        "        part=\"snippet\",\n",
        "        videoId=video_id,\n",
        "        maxResults=100,\n",
        "        order=\"relevance\"\n",
        "    ).execute()\n",
        "\n",
        "    comments = []\n",
        "    for item in response[\"items\"]:\n",
        "      comment_text = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "      likes = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n",
        "      comment_text = unescape(comment_text)\n",
        "      comments.append({\"comment\": comment_text, \"num_of_likes\": likes})\n",
        "\n",
        "      #if 'nextPageToken' in response:\n",
        "      #  response = youtube.commentThreads().list(\n",
        "      #      part=\"snippet\",\n",
        "      #      videoId=video_id,\n",
        "      #      maxResults=100,\n",
        "      #      order=\"relevance\"\n",
        "      #      pageToken = response['nextPageToken']\n",
        "      #  ).execute()\n",
        "      #else:\n",
        "      #  break\n",
        "      comments_df = pd.DataFrame(comments).sort_values(by=['num_of_likes'], ascending=False, ignore_index=True)\n",
        "      comments_df.to_csv(\"comments.csv\", index=False)\n",
        "    if len(comments) == 0:\n",
        "      return comments\n",
        "    return comments_df\n",
        "  except HttpError as error:\n",
        "    print(f\"An HTTP error {error.http_status} occurred:\\n {error.content}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "FM03mM0F3coO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_details(video_id):\n",
        "  \"\"\"\n",
        "  Function to get details from a YouTube video.\n",
        "  Returns a tuple for title, channel, tags, views, likes.\n",
        "  \"\"\"\n",
        "  youtube = build('youtube', 'v3', developerKey=DEVELOPER_KEY)\n",
        "  request = youtube.videos().list(part='snippet,statistics', id=video_id)\n",
        "  details = request.execute()\n",
        "  thumbnail_url = details['items'][0]['snippet']['thumbnails']['high']['url']\n",
        "  channel = details['items'][0]['snippet']['channelTitle']\n",
        "  title = details['items'][0]['snippet']['title']\n",
        "  tags = details['items'][0]['snippet'].get('tags')\n",
        "  likes = int(details['items'][0]['statistics']['likeCount'])\n",
        "  views = int(details['items'][0]['statistics']['viewCount'])\n",
        "  videodate = details['items'][0]['snippet']['publishedAt']\n",
        "  return channel, title, tags, likes, views, thumbnail_url, videodate\n"
      ],
      "metadata": {
        "id": "V52WW_sXqlM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_captions(video_url, video_id):\n",
        "  \"\"\"\n",
        "  Function to get audio captions in 'en' (English) from a YouTube video.\n",
        "  Either from the YouTube subtitles if they exist, or from Assembly AI.\n",
        "  Uses Gemini to format the raw audio captions, and returns a string.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    yt = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
        "    captions = ''\n",
        "    for i in yt:\n",
        "      captions += i['text']+\" \"\n",
        "  except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(f\"Using Assembly AI instead...\")\n",
        "    !yt-dlp --get-url -f bestaudio $VIDEO_URL > audio.txt\n",
        "    with open('audio.txt', 'r') as file:\n",
        "      AUDIO_URL = file.read()\n",
        "    config = aai.TranscriptionConfig(auto_highlights=True)\n",
        "    transcriber = aai.Transcriber()\n",
        "    transcript = transcriber.transcribe(AUDIO_URL, config)\n",
        "    captions = transcript.text\n",
        "\n",
        "  # Using Gemini to format the raw audio captions\n",
        "  with open(\"prompts/reformat.prompt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "  prompt_template = PromptTemplate.from_template(template=text, template_format=\"jinja2\")\n",
        "  prompt: str = prompt_template.format(captions=captions)\n",
        "  response = model.generate_content(prompt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "  audio_captions_formatted = response.text\n",
        "\n",
        "  return audio_captions_formatted"
      ],
      "metadata": {
        "id": "bxpHDLCunUQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **STEP 2 | Extracting the claims to fact-check from Video audio**\n",
        "\n",
        "Functions to extract the top claims made in the video. We will be using Google's Gemini with robust prompt engineering - leveraging the LangChain library.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JLJ29nrg5UHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_claims(video_url):\n",
        "  \"\"\"\n",
        "  Function to extract the top claims that should be fact-checked.\n",
        "  Uses Gemini with the claims.prompt prompt.\n",
        "  Returns a tuple with: title, channel, thumbnail_url, claims, captions.\n",
        "      claims is a json object with fields 'claim', 'questions', 'passage', 'relevance'\n",
        "  \"\"\"\n",
        "  video_id = extract.video_id(video_url)\n",
        "  channel, title, _, _, _, thumbnail_url, videodate = get_video_details(video_id)\n",
        "  videodate = datetime.strptime(videodate[:10], '%Y-%m-%d').strftime(\"%Y-%m-%d\")\n",
        "  audio_captions_formatted = get_captions(video_url, video_id)\n",
        "\n",
        "  # Using the prompt template and calling Gemini\n",
        "  with open(\"prompts/claims.prompt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(template=text, template_format=\"jinja2\")\n",
        "  claims_prompt: str = prompt_template.format(\n",
        "      todaydate=date.today().strftime(\"%Y-%m-%d\"),\n",
        "      videodate=videodate,\n",
        "      channel=channel,\n",
        "      title=title,\n",
        "      captions=audio_captions_formatted,\n",
        "      )\n",
        "  response = model.generate_content(claims_prompt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "\n",
        "  json_promt = \"Make sure the following text can be read directly by json.loads(): <<< \" + response.text + \" >>>. '\\\n",
        "  Don't output anything else than your version of the text.\"\n",
        "  response = model.generate_content(json_promt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "  claims = json.loads(response.text[8:-4])\n",
        "\n",
        "  return title, channel, thumbnail_url, claims, audio_captions_formatted"
      ],
      "metadata": {
        "id": "M4xkC85HC0h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **STEP 3 | Fact-checking the claims by cross-checking reliable sources**\n",
        "\n",
        "Cross-reference claims with reliable sources, and classify claims into true, unsure and false (ideally with links / sources).\n",
        "\n",
        "\n",
        "This step leverages Data Commons, the Google FactCheck Claim Search API, the Wikipedia API and Google Search.\n",
        "\n",
        "The LLM is called several times throughout this step via robust prompt engineering to interpret, cross-reference, and ultimately classify the claims.  \n",
        "\n",
        "---\n",
        "\n",
        "[3] Radhakrishnan, Prashanth, et al. \"Knowing When to Ask--Bridging Large Language Models and Data.\" arXiv preprint arXiv:2409.13741 (2024).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "D_LDf7hiDUAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our reliable sources for Fact-Checking\n",
        "# 1 - Google Fact-Check API\n",
        "GOOGLE_FACT_CHECK_API_KEY = userdata.get('GFC_API_KEY')\n",
        "GOOGLE_FACT_CHECK_URL = 'https://factchecktools.googleapis.com/v1alpha1/claims:search'\n",
        "\n",
        "# 2 - Wikipedia\n",
        "wikipedia.set_lang('en')\n",
        "\n",
        "# 3 - Google Search\n",
        "GOOGLE_USER_AGENT = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36\""
      ],
      "metadata": {
        "id": "gufqI6AqsYK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_googlefacts(claim):\n",
        "  \"\"\"\n",
        "  Function to call the Google Fact Check API\n",
        "  Returns the claim reviews in the Claim Review structured format\n",
        "  https://developers.google.com/search/docs/appearance/structured-data/factcheck\n",
        "  \"\"\"\n",
        "\n",
        "  params = {\n",
        "      'query': claim,\n",
        "      'key': GOOGLE_FACT_CHECK_API_KEY\n",
        "  }\n",
        "  response = requests.get(GOOGLE_FACT_CHECK_URL, params=params)\n",
        "  if response.status_code == 200:\n",
        "      data = response.json()\n",
        "      return data.get('claims', [])\n",
        "  else:\n",
        "      return None\n",
        "\n",
        "def check_googlefacts(claim):\n",
        "  \"\"\"\n",
        "  Function to process the Google Fact Check claim reviews\n",
        "  Returns a string = concatenation of relevant results\n",
        "  \"\"\"\n",
        "  results = call_googlefacts(claim)\n",
        "  summary = ''\n",
        "  if results:\n",
        "      for i, r in enumerate(results):\n",
        "          source = r.get('claimReview', [{}])[0].get('publisher').get('name')\n",
        "          url = r.get('claimReview', [{}])[0].get('url')\n",
        "          claimant = r.get('claimant')\n",
        "          date = r.get('claimDate')\n",
        "          text = r.get('text')\n",
        "          truthfulness = r.get('claimReview', [{}])[0].get('textualRating')\n",
        "          summary += f\"Source #{i+1}: {source} at {url}.\\n     Claimant: {claimant}\\n     Description: {text}\\n     Truthfulness: {truthfulness}\\n\\n\"\n",
        "\n",
        "  return summary\n",
        "\n",
        "def check_googlesearch(claim, limit=2):\n",
        "  \"\"\"\n",
        "  Function to call the Google Search API\n",
        "  Calls Gemini to process the results and generate a summary\n",
        "  Returns a string = concatenation of relevant results\n",
        "  \"\"\"\n",
        "  urls = list(g.search(claim, stop=limit, lang='en'))\n",
        "  headers = {\"user-agent\": GOOGLE_USER_AGENT}\n",
        "  summary = ''\n",
        "  for url in urls:\n",
        "    session = requests.Session()\n",
        "    website = session.get(url, headers=headers)\n",
        "    web_soup = BeautifulSoup(website.text, 'html.parser')\n",
        "    summary_promt = \"I need you to give me the key information contained in a specific webpage article. I will provide you with html code. '\\\n",
        "    Do not describe the webpage or the article. Focus on extracting the key claims, and summarizing the information the page is giving in a precise, comprehensive yet concise way. '\\\n",
        "    Your answer should ideally help answer the question: \" + claim + \".\\n'\\\n",
        "    HTLM Code: <<< \" + str(web_soup) + \" >>>\"\n",
        "    retry_count = 0\n",
        "    k = random.randint(0, 1)\n",
        "    time.sleep(k)\n",
        "    while retry_count < 3:\n",
        "      try:\n",
        "         response = model.generate_content(summary_promt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "         summary += f\"Source: {url}.\\nDescription: {response.text}\\n\"\n",
        "         retry_count = 3\n",
        "      except Exception as e:\n",
        "         time.sleep(2)\n",
        "      retry_count += 1\n",
        "    session.close()\n",
        "  return summary\n",
        "\n",
        "def check_wikipedia(claim):\n",
        "  \"\"\"\n",
        "  Function to call the Wikipedia API and process the results\n",
        "  Returns a string = concatenation/summary of relevant Wikipedia articles\n",
        "  \"\"\"\n",
        "  summary = ''\n",
        "  search_results = wikipedia.search(claim)\n",
        "  for r in search_results[:2]:\n",
        "    try:\n",
        "      call = wikipedia.page(r)\n",
        "      summary += f\"From the \\\"{r}\\\" Wikipedia page ({call.url}): \"+ call.content +\"\\n\\n\"\n",
        "    except wikipedia.exceptions.DisambiguationError:\n",
        "      summary += ''\n",
        "    except wikipedia.exceptions.PageError:\n",
        "      summary += ''\n",
        "    except wikipedia.exceptions.WikipediaException:\n",
        "      summary += ''\n",
        "    except Exception:\n",
        "      summary += ''\n",
        "  return summary"
      ],
      "metadata": {
        "id": "sANlwtp9XIT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_claim_summary(claim, questions):\n",
        "  \"\"\"\n",
        "  Function to get the final say on a specific claim.\n",
        "  Calls Gemini with the factcheck.prompt prompt.\n",
        "  Returns json object with fields: `claim`, `verdict`, `reason`, `sources`.\n",
        "  \"\"\"\n",
        "  summary_gfc = ''\n",
        "  summary_search = ''\n",
        "  for q in questions:\n",
        "    gfc = check_googlefacts(q)\n",
        "    if gfc:\n",
        "      summary_gfc += gfc + \"\\n\"\n",
        "  for q in questions:\n",
        "    search = check_googlesearch(q)\n",
        "    summary_search += search + \"\\n\"\n",
        "  summary_wiki = check_wikipedia(claim)\n",
        "\n",
        "  with open(\"prompts/factcheck.prompt\", \"r\") as f:\n",
        "        text = f.read()\n",
        "  prompt_template = PromptTemplate.from_template(template=text, template_format=\"jinja2\")\n",
        "  fact_check_prompt: str = prompt_template.format(\n",
        "      claim=claim,\n",
        "      report_GFC=summary_gfc + \"\\n\" + summary_search,\n",
        "      report_wiki=summary_wiki,\n",
        "      )\n",
        "  k = random.randint(3, 5)\n",
        "  time.sleep(k)\n",
        "  retry_count = 0\n",
        "  while retry_count < 3:\n",
        "      try:\n",
        "         response = model.generate_content(fact_check_prompt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "         return response.text\n",
        "      except Exception as e:\n",
        "         #print(f\"Error: {e}\")\n",
        "         k = random.randint(1, 3)\n",
        "         time.sleep(k)\n",
        "         retry_count += 1\n",
        "  return None"
      ],
      "metadata": {
        "id": "BeYIhFT0hW3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **TRUTH SLEUTH AGENT | Putting it all together & Generating the Fact-Check Report**\n",
        "\n",
        "Generating the final report, leveraging Markdown for formatting. Option to skip printing the \"Unsure\" claims.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "9m5eeMQpFShD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_report(video_url, skip_unsure = True, print_report = True):\n",
        "\n",
        "  # Extracting the claims\n",
        "  title, channel, thumbnail_url, claims, captions = extract_claims(video_url)\n",
        "\n",
        "  # Formatting\n",
        "  color = {\"question\": \"black\", \"true\": \"MediumSpringGreen\", \"partly true\": \"LightGreen\", \"partly false\": \"lightcoral\", \"false\": \"red\", \"unsure\": \"grey\"}\n",
        "  capped = {\"true\": \"TRUE\", \"partly true\": \"PARTLY TRUE\", \"partly false\": \"PARTLY FALSE\", \"false\": \"FALSE\", \"unsure\": \"UNSURE\"}\n",
        "  delimiter = \"\\n\"+\"_\"*100+\"\\n\"\n",
        "\n",
        "  # Generating Report Header\n",
        "  thumb = Image.open(requests.get(thumbnail_url, stream=True).raw)\n",
        "  formatted_title1 = f\"<font size='+2' color='Bisque'><blockquote>📓📓 🔍 **TRUTH SLEUTH FACT-CHECK REPORT** 🔍 📓📓</blockquote></font>\"\n",
        "  formatted_title2 = f\"<font size='+2' color='white'><blockquote>**{title}** by {channel}</blockquote></font>\"\n",
        "\n",
        "  # Getting each claim's verdict + Formatting again\n",
        "  report = \"\"\n",
        "  formatted_report = \"\"\n",
        "  for i, c in enumerate(claims[\"claims\"]):\n",
        "    clear_output()\n",
        "    print(f\"Going over claim #{i+1}: {c['claim']}\")\n",
        "    text = get_claim_summary(c['claim'], c['questions'])\n",
        "    if text == None:\n",
        "      continue\n",
        "    text = text.replace(\"$\", \"USD \")\n",
        "\n",
        "    try:\n",
        "      verdict = json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "      verdict = json.loads(text[8:-4])\n",
        "    if (verdict.get('verdict') == \"unsure\" and skip_unsure):\n",
        "      continue\n",
        "\n",
        "    formatted_claim = f\"<font size='+1' color='white'><blockquote>**• {c['claim']}**</blockquote></font>\"\n",
        "    formatted_verdict = f\"<font size='+1' color='{color[verdict.get('verdict')]}'><blockquote>**{capped[verdict.get('verdict')]}**\\n\\n</blockquote></font>\"\n",
        "    links = \"\"\n",
        "    for l in verdict.get('sources'):\n",
        "      links += \"\\n\\n• \" + l\n",
        "    report += \"Claim: \" + c['claim'] + \"This is: \"+capped[verdict.get('verdict')] + \"\\n\"+verdict.get('reason') + \"\\nSources: \" + str(verdict.get('sources')) + \"\\n\\n\"\n",
        "    formatted_report += delimiter + formatted_claim + formatted_verdict + verdict.get('reason') + links\n",
        "\n",
        "  clear_output()\n",
        "  if print_report:\n",
        "    display(Markdown(formatted_title1 + formatted_title2))\n",
        "    display(thumb)\n",
        "    display(Markdown(formatted_report))\n",
        "    display(Markdown(delimiter))\n",
        "\n",
        "  return title, channel, claims, captions, report, formatted_report"
      ],
      "metadata": {
        "id": "2gk083chvxKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "title, channel, claims, captions, fact_check_report, formatted_fact_check_report = generate_report(VIDEO_URL, True, True)"
      ],
      "metadata": {
        "id": "1qAWy_eBMCHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **TREND BENDER AGENT | Influencing the comment section**\n",
        "\n",
        "Ultimately, could we even change people's minds? In particular, could our conversational agent influence the comment section of a video, keep the discussion away from hate/conspiracy and protect people from scams?\n",
        "\n",
        "**Theme 1**: Dangerous Diet Culture / Weight Loss videos.\n",
        "**Theme 2**: Manosphere & Misogynistic videos.  \n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "oay7AaNg4AL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose your theme:\n",
        "THEME = \"manosphere\" #\"manosphere\", \"health\", \"diet\"\n",
        "CONTEXT_WIKI_PAGES = {\n",
        "    \"manosphere\": [\"gender role\", \"intimate relationship\", \"violence against women\"],\n",
        "    \"health\": [\"Quackery\", \"Pseudoscience\", \"Alternative medicine\"],\n",
        "    \"diet\": [\"Eating disorder\", \"Body image\"]\n",
        "}\n",
        "\n",
        "CONTEXT_SEARCH = {\n",
        "    \"manosphere\": [\"understanding Andrew Tate’s appeal to lost men\", \"Do Men Actually Not Want to Date Intelligent Women?\", \"The Power Couple Effect: Why Working Together Yields Success\"],\n",
        "    \"health\": [\"The Menace of Wellness Influencers\", \"Health misinformation is rampant on social media\", \"Is fear mongering just as bad as diet culture?\"],\n",
        "    \"diet\": [\"Diet Culture\", \"A review of current knowledge about the impact of diet on mental health\", \"Can What We Eat Determine How We Think?\", \"How misinformation is making us fear our food\"]\n",
        "}\n",
        "\n",
        "# Get context from wikipedia\n",
        "def get_context(theme):\n",
        "  \"\"\"\n",
        "  Function to call the Wikipedia API / Search API to get context from the theme's corresponding Wikipedia pages\n",
        "  as well as key searches from Google\n",
        "  Returns a string = concatenation/summary of relevant Wikipedia articles\n",
        "  \"\"\"\n",
        "  summary = ''\n",
        "  for r in CONTEXT_WIKI_PAGES[theme]:\n",
        "    try:\n",
        "      call = wikipedia.page(r)\n",
        "      summary += f\"From the \\\"{r}\\\" Wikipedia page ({call.url}): \"+ call.summary +\"\\n\"+\"_\"*150+\"\\n\\n\"\n",
        "    except wikipedia.exceptions.DisambiguationError:\n",
        "      summary += ''\n",
        "    except wikipedia.exceptions.PageError:\n",
        "      summary += ''\n",
        "    except wikipedia.exceptions.WikipediaException:\n",
        "      summary += ''\n",
        "    except Exception:\n",
        "      summary += ''\n",
        "  for r in CONTEXT_SEARCH[theme]:\n",
        "    summary += check_googlesearch(r) +\"\\n\"+\"_\"*150+\"\\n\\n\"\n",
        "  return summary"
      ],
      "metadata": {
        "id": "5Z1sP3pI4zI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_trend(channel, title, captions, comments, context):\n",
        "  \"\"\"\n",
        "  Function to extract the trends from the video's comment section.\n",
        "  Returns a string.\n",
        "  \"\"\"\n",
        "  # Using the prompt template and calling Gemini\n",
        "  with open(\"prompts/trend.prompt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(template=text, template_format=\"jinja2\")\n",
        "  trend_prompt: str = prompt_template.format(\n",
        "      channel=channel,\n",
        "      title=title,\n",
        "      captions=captions,\n",
        "      comments=comments,\n",
        "      context=context,\n",
        "      )\n",
        "  response = model.generate_content(trend_prompt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "\n",
        "  return response.text"
      ],
      "metadata": {
        "id": "fccIPZudTKNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "context = get_context(THEME)\n",
        "comments = get_comments(VIDEO_URL)\n",
        "trends = extract_trend(channel, title, captions, comments, context)\n",
        "\n",
        "sample_comments = ''\n",
        "for i, c in enumerate(comments.comment[:10]):\n",
        "  sample_comments += \"Comment #\" + str(i+1) + c + \"\\n\"\n",
        "\n",
        "clear_output()\n",
        "print(context)"
      ],
      "metadata": {
        "id": "yrs2ieoq0KV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comment Prompt Templates\n",
        "\n",
        "!curl -L -o prompts/comment_1A.prompt \"https://drive.google.com/uc?export=download&id=1Wc_CbJNHffAuh6xhAsOK329iLbqeWrsb\"\n",
        "!curl -L -o prompts/comment_1AB.prompt \"https://drive.google.com/uc?export=download&id=1MO3kA51ukw8w8jVtH7IzdYVArlXREzAJ\"\n",
        "!curl -L -o prompts/comment_1ABC.prompt \"https://drive.google.com/uc?export=download&id=1FEZ_ZOhAkLCIfn4wi9avsqwwnbGMVnbl\"\n",
        "!curl -L -o prompts/comment_1ABCD.prompt \"https://drive.google.com/uc?export=download&id=10PFYXZ5DDA74IqIH-0bv3Ew8SI_GajOS\"\n",
        "!curl -L -o prompts/comment_2A.prompt \"https://drive.google.com/uc?export=download&id=1p2alhX0BPgK9ghCF42RyKjcV7D4IcFtF\"\n",
        "!curl -L -o prompts/comment_2AB.prompt \"https://drive.google.com/uc?export=download&id=1Y7l4xO1Rc4vAHx1_oyNI81mSnF2bIgg7\"\n",
        "!curl -L -o prompts/comment_2ABC.prompt \"https://drive.google.com/uc?export=download&id=1XZGlLU1NAUOkWJOkycBwlIV3oOGhL3R5\"\n",
        "!curl -L -o prompts/comment_2ABCD.prompt \"https://drive.google.com/uc?export=download&id=1nXzRNnQkg7bDVjRyyZvWvWjqJp2v5DeO\"\n",
        "!curl -L -o prompts/comment_3ABC.prompt \"https://drive.google.com/uc?export=download&id=1fqDj5UOdNLnOk641qRnWDzKSDJxwonJc\"\n",
        "!curl -L -o prompts/comment_3ABCD.prompt \"https://drive.google.com/uc?export=download&id=1QqLxOJsqkM_VktHAb-sbr0Vi85GIjsnX\"\n",
        "\n",
        "!curl -L -o prompts/comment_reply_2ABCD.prompt \"https://drive.google.com/uc?export=download&id=16Un9vw2J4cf_k1nCiOBLB7j4-3np9P7M\"\n",
        "clear_output()\n",
        "print(\"Comment prompts have been uploaded in the 'prompt' folder and are ready to be used!\")"
      ],
      "metadata": {
        "id": "zVRRv6qnUPf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2d6cf5-c0ea-4f80-8277-a9951c191eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment prompts have been uploaded in the 'prompt' folder and are ready to be used!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation & Feedback Prompt Templates\n",
        "!curl -L -o prompts/comment_ABCD_feedback.prompt \"https://drive.google.com/uc?export=download&id=1hSh6f6KiIrviL59JkglNM1chBKQnlSGi\"\n",
        "!curl -L -o prompts/evaluation.prompt \"https://drive.google.com/uc?export=download&id=1ELsBKmxR7y9eXqkX11UjVpaxmqqR6sac\"\n",
        "\n",
        "!curl -L -o prompts/comment_reply_ABCD_feedback.prompt \"https://drive.google.com/uc?export=download&id=1n_AdEgi_HO96gE4dRzeKBQZFUPt3BDLq\"\n",
        "!curl -L -o prompts/evaluation_reply.prompt \"https://drive.google.com/uc?export=download&id=1kXxNT0ZI5QMWV_wrVBequpVtVghPku6N\"\n",
        "\n",
        "clear_output()\n",
        "print(\"Self-evaluation prompts have been uploaded in the 'prompt' folder and are ready to be used!\")"
      ],
      "metadata": {
        "id": "HAcROrFwj6jr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "243ac946-4e5d-42d8-d42c-eb2362898b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-evaluation prompts have been uploaded in the 'prompt' folder and are ready to be used!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prompting(fileloc, channel, title, captions, theme, comments, trends, fact_check_report, context, response=None, feedback=None, specific_comment=None):\n",
        "  \"\"\"\n",
        "  Function to turn the downloaded prompt file into a usable prompt string, using LangChain's prompt template.\n",
        "  Returns a string.\n",
        "  \"\"\"\n",
        "  with open(fileloc, \"r\") as f:\n",
        "    text = f.read()\n",
        "  prompt_template = PromptTemplate.from_template(template=text, template_format=\"jinja2\")\n",
        "  prompt: str = prompt_template.format(\n",
        "      theme=theme,\n",
        "      channel=channel,\n",
        "      title=title,\n",
        "      captions=captions,\n",
        "      trends=trends,\n",
        "      comments=comments,\n",
        "      fact_check=fact_check_report,\n",
        "      context=context,\n",
        "      response=response,\n",
        "      feedback=feedback,\n",
        "      comment_to_focus=specific_comment,\n",
        "      )\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "4vZhT8DD0Gjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experimenting with different kinds of prompts...\n",
        "PROMPT_LOCS = [\n",
        "               #\"prompts/comment_1A.prompt\", \"prompts/comment_1AB.prompt\", \"prompts/comment_1ABC.prompt\",\n",
        "               #\"prompts/comment_1ABCD.prompt\",\n",
        "               #\"prompts/comment_2A.prompt\", \"prompts/comment_2AB.prompt\",\n",
        "               #\"prompts/comment_2ABC.prompt\",\n",
        "               \"prompts/comment_2ABCD.prompt\",\n",
        "               #\"prompts/comment_3ABC.prompt\",\n",
        "               \"prompts/comment_3ABCD.prompt\"\n",
        "               ]\n",
        "\n",
        "def experiment_generate_comments(channel, title, captions, theme, comments, context, trends, fact_check_report):\n",
        "  \"\"\"\n",
        "  Function to generate an influential comment for the video's comment section.\n",
        "  Returns a string.\n",
        "  \"\"\"\n",
        "  # Using the prompt template and calling Gemini\n",
        "  all_prompts = {}\n",
        "  for loc in PROMPT_LOCS:\n",
        "    prompt = prompting(loc, channel, title, captions, theme, comments, trends, fact_check_report, context)\n",
        "    all_prompts[loc.split(\"/\")[1].split(\".\")[0]] = prompt\n",
        "\n",
        "  result = \"\"\n",
        "  for p, prompt in all_prompts.items():\n",
        "    print(\"Prompt: \" + p)\n",
        "    result += \"Prompt: \" + p + \"\\n\"\n",
        "    for i in range(3):\n",
        "      response = model.generate_content(prompt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "      print(i+1)\n",
        "      print(response.text + \"\\n\")\n",
        "      result += str(i+1) + \"\\n\" + response.text + \"\\n\"\n",
        "      time.sleep(2)\n",
        "  return result\n",
        "\n",
        "def experiment_generate_comments_with_feedback(channel, title, captions, theme, comments, context, trends, fact_check_report):\n",
        "  \"\"\"\n",
        "  Function to generate an influential comment for the video's comment section.\n",
        "  Interacts with Grader Agent to improve on first output.\n",
        "  Returns two strings: one with the comments, one with the feedbacks.\n",
        "  \"\"\"\n",
        "  # Using the prompt template and calling Gemini\n",
        "  all_prompts = {}\n",
        "  for loc in PROMPT_LOCS:\n",
        "    prompt = prompting(loc, channel, title, captions, theme, comments, trends, fact_check_report, context)\n",
        "    all_prompts[loc.split(\"/\")[1].split(\".\")[0]] = prompt\n",
        "\n",
        "  result = \"\"\n",
        "  feedback = \"\"\n",
        "  for p, prompt in all_prompts.items():\n",
        "    print(\"Prompt: \" + p)\n",
        "    result += \"Prompt: \" + p + \"\\n\"\n",
        "    feedback += \"Prompt: \" + p + \"\\n\"\n",
        "    for i in range(3):\n",
        "      response = model.generate_content(prompt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "      print(i+1)\n",
        "      #print(response.text + \"\\n\")\n",
        "      result += str(i+1) + \"\\n\" + response.text + \"\\n\"\n",
        "      time.sleep(2)\n",
        "      evaluation_prompt = prompting(\"prompts/evaluation.prompt\", channel, title, captions, theme, comments, trends, fact_check_report, context, response=response.text)\n",
        "      grading = model.generate_content(evaluation_prompt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "      improve_prompt = prompting(\"prompts/comment_ABCD_feedback.prompt\", channel, title, captions, theme, comments, trends, fact_check_report, context, response=response.text, feedback=grading.text)\n",
        "      improved_response = model.generate_content(improve_prompt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "      print(\"After feedback: \\n\" + improved_response.text + \"\\n\")\n",
        "      result += \"After feedback: \" + improved_response.text + \"\\n\"\n",
        "      feedback += str(i+1) + \"\\n\" + grading.text + \"\\n\"\n",
        "  return result, feedback"
      ],
      "metadata": {
        "id": "xzgpb03azmiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = experiment_generate_comments(channel, title, captions, THEME, sample_comments, context, trends, fact_check_report)"
      ],
      "metadata": {
        "id": "e4z4adQXVmRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, feedback = experiment_generate_comments_with_feedback(channel, title, captions, THEME, sample_comments, context, trends, fact_check_report)"
      ],
      "metadata": {
        "id": "1rs2X5hDovEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **COMMENT EXPERIMENTS | Interacting with *real* users**\n",
        "\n",
        "Now let's actually post under some Youtube videos and observe *real* users' reactions!\n",
        "\n",
        "**Theme 1**: Dangerous Diet Culture / Weight Loss videos.\n",
        "**Theme 2**: Manosphere & Misogynistic videos.  \n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "n_HsB9Wh6cJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the top comments...\n",
        "\n",
        "def get_individual_comments(video_id, comments_order, max_results=5):\n",
        "    youtube = build(\"youtube\", \"v3\", developerKey=DEVELOPER_KEY)\n",
        "    comments = []\n",
        "    next_page_token = None\n",
        "    while len(comments) < max_results:\n",
        "        request = youtube.commentThreads().list(\n",
        "            part=\"snippet\",\n",
        "            videoId=video_id,\n",
        "            maxResults=min(100, max_results - len(comments)),\n",
        "            order=comments_order,\n",
        "            pageToken=next_page_token,\n",
        "            textFormat=\"plainText\",\n",
        "        )\n",
        "        try:\n",
        "            response = request.execute()\n",
        "            comments.extend(\n",
        "                [\n",
        "                    (\n",
        "                        item[\"id\"],  # Extract the comment thread ID\n",
        "                        unescape(item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"])\n",
        "                    )\n",
        "                    for item in response[\"items\"]\n",
        "                ]\n",
        "            )\n",
        "            next_page_token = response.get(\"nextPageToken\")\n",
        "            if not next_page_token:\n",
        "                break\n",
        "\n",
        "        except HttpError as error:\n",
        "            print(f\"An HTTP error {error.http_status} occurred:\\n {error.content}\")\n",
        "\n",
        "    return comments[:max_results]\n",
        "\n",
        "def get_top_recent_comments(video_id, max_results):\n",
        "    order=\"time\"\n",
        "    return get_individual_comments(video_id, order, max_results)\n",
        "\n",
        "def get_top_liked_comments(video_id, max_results):\n",
        "    order=\"relevance\"\n",
        "    return get_individual_comments(video_id, order, max_results)"
      ],
      "metadata": {
        "id": "8kn9npUg6bTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_general_comment_with_feedback(channel, title, captions, theme, comments, context, trends, fact_check_report):\n",
        "  \"\"\"\n",
        "  Function to generate an influential comment for the video's comment section.\n",
        "  Interacts with Grader Agent to improve on first output.\n",
        "  Returns string of final improved response that should be used for posting.\n",
        "  \"\"\"\n",
        "  prompt = prompting(\"prompts/comment_2ABCD.prompt\", channel, title, captions, theme, comments, trends, fact_check_report, context)\n",
        "\n",
        "  # First response\n",
        "  response = model.generate_content(prompt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "  time.sleep(2)\n",
        "\n",
        "  # Self-evaluating\n",
        "  evaluation_prompt = prompting(\"prompts/evaluation.prompt\", channel, title, captions, theme, comments, trends, fact_check_report, context, response=response.text)\n",
        "  grading = model.generate_content(evaluation_prompt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "\n",
        "  # Improving on first reponse\n",
        "  improve_prompt = prompting(\"prompts/comment_ABCD_feedback.prompt\", channel, title, captions, theme, comments, trends, fact_check_report, context, response=response.text, feedback=grading.text)\n",
        "  improved_response = model.generate_content(improve_prompt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "\n",
        "  return improved_response.text\n",
        "\n",
        "def generate_reply_to_comment_with_feedback(channel, title, captions, theme, comments, context, trends, fact_check_report, comment_to_reply):\n",
        "  \"\"\"\n",
        "  Function to generate an influential comment for the video's comment section, as a reply to a chosen comment.\n",
        "  Interacts with Grader Agent to improve on first output.\n",
        "  Returns string of final improved response that should be used for posting.\n",
        "  \"\"\"\n",
        "  prompt = prompting(\"prompts/comment_reply_2ABCD.prompt\", channel, title, captions, theme, comments, trends, fact_check_report, context, specific_comment=comment_to_reply)\n",
        "\n",
        "  # First response\n",
        "  response = model.generate_content(prompt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "  time.sleep(2)\n",
        "\n",
        "  # Self-evaluating\n",
        "  evaluation_prompt = prompting(\"prompts/evaluation_reply.prompt\", channel, title, captions, theme, comments, trends, fact_check_report, context, response=response.text, specific_comment=comment_to_reply)\n",
        "  grading = model.generate_content(evaluation_prompt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "\n",
        "  # Improving on first reponse\n",
        "  improve_prompt = prompting(\"prompts/comment_reply_ABCD_feedback.prompt\", channel, title, captions, theme, comments, trends, fact_check_report, context, response=response.text, feedback=grading.text, specific_comment=comment_to_reply)\n",
        "  improved_response = model.generate_content(improve_prompt, safety_settings=GEM_SAFETY_SETTINGS)\n",
        "\n",
        "  return improved_response.text\n"
      ],
      "metadata": {
        "id": "Yck7mHAV80kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VIDEO_THEMES = {\"YddQ66BcYIU\": \"manosphere\",\n",
        "                \"8Jl4zm5ftCM\": \"manosphere\",\n",
        "                \"VYvIenW1XzE\": \"diet\",\n",
        "                \"Ou2IsE7BoFI\": \"diet\"}\n",
        "\n",
        "VIDEOS = [\"https://www.youtube.com/watch?v=YddQ66BcYIU\", \"https://www.youtube.com/watch?v=Q0wKXhhOZZU\",\n",
        "          \"https://www.youtube.com/watch?v=VYvIenW1XzE\", \"https://www.youtube.com/watch?v=Ou2IsE7BoFI\"]\n",
        "\n",
        "def initiate_csv(name_csv):\n",
        "    with open(name_csv, \"a\", newline=\"\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\"video_url\", \"video_id\", \"user_comment_id\", \"user_comment_text\", \"generated_comment\"])\n",
        "\n",
        "def populate_csv(name_csv, row_list):\n",
        "    with open(name_csv, \"a\", newline=\"\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(row_list)\n",
        "\n",
        "def from_start_to_finish(video_url):\n",
        "  \"\"\"\n",
        "  This function does three things:\n",
        "    - Generates a comment to the video (opinion)\n",
        "    - Reads top 5 most recent comments and generates a response to those comments\n",
        "    - Reads top 5 most liked comments and generates a response to those comments\n",
        "\n",
        "  To do so, it:\n",
        "    - Extracts basic video data (title, channel, comments, captions)\n",
        "    - Generates a Fact-Check report using Truth Sleuth\n",
        "    - Gathers articles into a themed corpus for context\n",
        "    - Generates convincing comments using Trend Bender\n",
        "  \"\"\"\n",
        "  ### --- ### --- ### BASICS + TRUTH SLEUTH AGENT ### --- ### --- ###\n",
        "  # Extract basics & Fact-Check report\n",
        "  video_id = extract.video_id(video_url)\n",
        "  channel, title, _, _, _, _, _ = get_video_details(video_id)\n",
        "  print(f\"Video: {title} from {channel} ... ... ...\")\n",
        "  initiate_csv(f\"generated_comment_{video_id}.csv\")\n",
        "\n",
        "  print(\"Truth Sleuth generating Fact-Check report ... ... ...\")\n",
        "  _, _, claims, captions, fact_check_report, report = generate_report(video_url, True, False)\n",
        "  print(\"Fact-Check report extracted!!\")\n",
        "\n",
        "  comments = get_comments(video_url)\n",
        "  sample_comments = ''\n",
        "  for i, c in enumerate(comments.comment[:10]):\n",
        "    sample_comments += \"Comment #\" + str(i+1) + c + \"\\n\"\n",
        "  clear_output()\n",
        "\n",
        "\n",
        "  # Extract trends & context\n",
        "  print(\"Now putting together themed corpus ... ... ...\")\n",
        "  theme = VIDEO_THEMES[video_id]\n",
        "  context = get_context(theme)\n",
        "  trends = extract_trend(channel, title, captions, comments, context)\n",
        "  clear_output()\n",
        "\n",
        "  #comment_rows = []\n",
        "  ### --- ### --- ###     TREND BENDER AGENT     ### --- ### --- ###\n",
        "  print(\"Over to Trend Bender agent to generate comments ... ... ...\")\n",
        "  # Part 1: Generate a general comment (improved version) and post it to the video\n",
        "  print(\"One general comment ... ... ...\")\n",
        "  comment_for_video = generate_general_comment_with_feedback(channel, title, captions, theme, sample_comments, context, trends, fact_check_report)\n",
        "  populate_csv(f\"generated_comment_{video_id}.csv\", [video_url, video_id, \"general\", \"\", comment_for_video])\n",
        "  #comment_rows.append([video_url, video_id, \"general\", \"\", comment_for_video])\n",
        "\n",
        "  # Part 2: Generate responses to 5 most recent comments and post them (1 response for 1 comment)\n",
        "  print(\"Five recent comments ... ... ...\")\n",
        "  comments_response = get_top_recent_comments(video_id, max_results=5)\n",
        "  for comment_id, comment_text in comments_response:\n",
        "      reply_to_comment = generate_reply_to_comment_with_feedback(channel, title, captions, theme, sample_comments, context, trends, fact_check_report, comment_text)\n",
        "      populate_csv(f\"generated_comment_{video_id}.csv\", [video_url, video_id, comment_id, comment_text, reply_to_comment])\n",
        "      k = random.randint(1, 3)\n",
        "      time.sleep(k)\n",
        "\n",
        "  # Part 3: Generate responses to top 5 comments and post them (1 response for 1 comment)\n",
        "  print(\"Five top comments ... ... ...\")\n",
        "  comments_response = get_top_liked_comments(video_id, max_results=5)\n",
        "  for comment_id, comment_text in comments_response:\n",
        "      reply_to_comment = generate_reply_to_comment_with_feedback(channel, title, captions, theme, sample_comments, context, trends, fact_check_report, comment_text)\n",
        "      populate_csv(f\"generated_comment_{video_id}.csv\", [video_url, video_id, comment_id, comment_text, reply_to_comment])\n",
        "      k = random.randint(1, 3)\n",
        "      time.sleep(k)\n",
        "\n",
        "  clear_output()\n",
        "  print(f\"Comments were generated for {title} from {channel} with the theme: '{theme}'!\")\n",
        "  display(Markdown(report))\n",
        "\n"
      ],
      "metadata": {
        "id": "9_GSp0EyLNvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def from_start_to_finish_reply(video_url, comment_to_reply_to, fact_check_report=None):\n",
        "  \"\"\"\n",
        "  This function does three things:\n",
        "    - Generates a response to a comment\n",
        "\n",
        "  To do so, it:\n",
        "    - Extracts basic video data (title, channel, comments, captions)\n",
        "    - Generates a Fact-Check report using Truth Sleuth\n",
        "    - Gathers articles into a themed corpus for context\n",
        "    - Generates convincing comments using Trend Bender\n",
        "  \"\"\"\n",
        "  ### --- ### --- ### BASICS + TRUTH SLEUTH AGENT ### --- ### --- ###\n",
        "  # Extract basics & Fact-Check report\n",
        "  video_id = extract.video_id(video_url)\n",
        "  channel, title, _, _, _, _, _ = get_video_details(video_id)\n",
        "  print(f\"Video: {title} from {channel} ... ... ...\")\n",
        "\n",
        "  if fact_check_report is None:\n",
        "    print(\"Truth Sleuth generating Fact-Check report ... ... ...\")\n",
        "    _, _, claims, captions, fact_check_report, _ = generate_report(video_url, True, False)\n",
        "    print(\"Fact-Check report extracted!!\")\n",
        "  else:\n",
        "    captions = get_captions(video_url, video_id)\n",
        "\n",
        "  comments = get_comments(video_url)\n",
        "  sample_comments = ''\n",
        "  for i, c in enumerate(comments.comment[:10]):\n",
        "    sample_comments += \"Comment #\" + str(i+1) + c + \"\\n\"\n",
        "  clear_output()\n",
        "\n",
        "\n",
        "  # Extract trends & context\n",
        "  print(\"Now putting together themed corpus ... ... ...\")\n",
        "  theme = VIDEO_THEMES[video_id]\n",
        "  context = get_context(theme)\n",
        "  trends = extract_trend(channel, title, captions, comments, context)\n",
        "  clear_output()\n",
        "\n",
        "  # Generate response\n",
        "  print(\"Responding to comment ... ... ...\")\n",
        "  reply_to_comment = generate_reply_to_comment_with_feedback(channel, title, captions, theme, sample_comments, context, trends, fact_check_report, comment_to_reply_to)\n",
        "\n",
        "  clear_output()\n",
        "  print(f\"Comments were generated for {title} from {channel} with the theme: '{theme}'!\")\n",
        "  print(reply_to_comment)\n",
        "\n",
        "  return reply_to_comment\n"
      ],
      "metadata": {
        "id": "qISASl2qEogX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from_start_to_finish(VIDEO_URL)"
      ],
      "metadata": {
        "id": "5xWgPNFbge3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment_vid = '''\n",
        "@Venom_Byte:\n",
        "Hold the line gentlemen\n",
        "\n",
        "@SUSleuth:\n",
        "Holding the line on what, exactly? The video's claim that men have \"STOPPED Pursuing Modern Women!\" ignores the complexities of modern relationships. Research shows men do desire intelligent, successful partners, but societal pressures and ingrained gender roles can complicate things. This video oversimplifies a nuanced issue. Let's examine the broader context of gender roles and relationships.\n",
        "\n",
        "@MoltenMetalGod7:\n",
        "@SUSleuth there’s nothing complex about it even if its desired if the ROI isn’t there it’s not worth it. There’s nothing to examine. Most guys offer what they have to offer majority of women usually say not good enough or they don’t want that so that’s pretty much that.\n",
        "\n",
        "@SUSleuth:\n",
        "@MoltenMetalGod7  I hear you –  \"ROI isn't there, it's not worth it\" is a very clear and understandable perspective.  The video frames \"success\" very narrowly, though.  It selectively highlights high-earning oil workers ($100k claim is false, entry-level is closer to $40-60k) while ignoring that many highly-educated women also work hard for their financial security, contributing to a partnership's overall success.  Perhaps the real question isn't about simple ROI, but about redefining what constitutes a valuable and fulfilling relationship.  Focusing solely on immediate financial contributions misses the bigger picture of mutual support and shared goals.\n",
        "\n",
        "@JoseLopez-eo4ze:\n",
        "@SUSleuth what's so complex about hypergamy & basic economics?\n",
        "'''\n",
        "reply = from_start_to_finish_reply(VIDEO_URL, comment_vid, fact_check_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_OxQum2Fcdc",
        "outputId": "364b1a9b-fafe-448b-f59c-9a6c51652772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comments were generated for Why Men Have STOPPED Pursuing Modern Women! from Rational Male Clips with the theme: 'manosphere'!\n",
            "@JoseLopez-eo4ze  Hypergamy and economics offer *part* of the picture, but the video simplifies things by focusing on \"marrying up\" educationally and financially, ignoring emotional connection. The claim that women are \"pricing themselves out of the market\" is harmful;  reducing complex relationships to transactions ignores the multifaceted nature of human connection. The video also falsely claims entry-level oil worker salaries are $100k+ (fact-check: closer to $40-60k).  A nuanced understanding of gender roles and relationship dynamics reveals a more complex reality. [https://en.wikipedia.org/wiki/Gender_role](https://en.wikipedia.org/wiki/Gender_role) [https://en.wikipedia.org/wiki/Intimate_relationship](https://en.wikipedia.org/wiki/Intimate_relationship) [https://albtriallawyers.com/how-much-do-oil-rig-workers-make/](https://albtriallawyers.com/how-much-do-oil-rig-workers-make/)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}